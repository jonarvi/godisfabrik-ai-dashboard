# ğŸ­ Godisfabrik 4.0: AI-Driven Produktions-Dashboard

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**AI-driven OEE & UnderhÃ¥lls-dashboard fÃ¶r en virtuell godisfabrik. Byggd med Python, Streamlit, SQL och Ollama (lokal LLM).**

## ğŸŒŸ Funktioner

*   ğŸ“Š **OEE-Dashboard:** Visualisering av Overall Equipment Effectiveness (OEE) fÃ¶r varje maskin, inklusive TillgÃ¤nglighet, Prestanda och Kvalitet.
*   ğŸ’¬ **AI-driven Chat:** FrÃ¥ga fabriken direkt om larm, trender, och underhÃ¥llsÃ¥tgÃ¤rder.
*   ğŸ“ˆ **Dynamiska Grafer:** Automatisk generering av stapeldiagram och andra visualiseringar baserat pÃ¥ dina frÃ¥gor.
*   ğŸ“… **Flexibla Datumfilter:** VÃ¤lj perioder fÃ¶r att analysera larm, produktion och OEE Ã¶ver tid.
*   ğŸ› ï¸ **Maskinspecifik Analys:** Drill-down i larm och sensordata fÃ¶r att hitta problem och identifiera lÃ¶sningar.
*   ğŸ¤– **Lokalt KÃ¶rande AI:** AnvÃ¤nder Ollama fÃ¶r att kÃ¶ra en LLM lokalt, vilket ger snabba och privata AI-analyser.
*   âš™ï¸ **Produktionsdata & Ordrar:** Simulera produktion, ordrar och OEE fÃ¶r att fÃ¥ en komplett bild av fabriken.

## ğŸš€ Kom igÃ¥ng (FÃ¶r andra anvÃ¤ndare)

FÃ¶r att kÃ¶ra denna applikation pÃ¥ din egen dator eller server, fÃ¶lj dessa steg:

1.  **Installera Docker & Docker Compose:**
    *   Du behÃ¶ver ha [Docker](https://www.docker.com/) och [Docker Compose](https://docs.docker.com/compose/) installerat pÃ¥ din maskin.

2.  **Installera NVIDIA Container Toolkit (Om du har GPU):**
    *   Om du har ett NVIDIA-grafikkort och vill anvÃ¤nda Ollama med GPU-acceleration, behÃ¶ver du installera [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).

3.  **Klona Repot:**
    ```bash
    git clone https://github.com/<DITT_GITHUB_ANVÃ„NDARNAMN>/godisfabrik-ai-dashboard.git
    cd godisfabrik-ai-dashboard
    ```
    (ErsÃ¤tt `<DITT_GITHUB_ANVÃ„NDARNAMN>` med ditt Github-anvÃ¤ndarnamn).

4.  **Skapa databasen:**
    *   Appen anvÃ¤nder en Microsoft SQL Server-databas. Docker Compose kommer att skapa den automatiskt.

5.  **Bygg och Starta Containrarna:**
    ```bash
    docker compose up -d --build
    ```
    Detta bygger applikationen och startar alla tjÃ¤nster (databas, AI, webbapp).

6.  **Ladda ner AI-modellen (Llama3):**
    *   Efter att containrarna startat, kÃ¶r fÃ¶ljande kommando fÃ¶r att ladda ner Llama 3-modellen (eller en annan modell som stÃ¶ds av Ollama):
        ```bash
        docker exec -it godisfabrik-ai ollama pull llama3
        ```

7.  **Ã–ppna i WebblÃ¤saren:** Ã–ppna din webblÃ¤sare och gÃ¥ till:
    ```
    http://localhost:8501
    ```
    (Om du kÃ¶r det pÃ¥ en server, se till att port 8501 Ã¤r Ã¶ppen och ersÃ¤tt "localhost" med serverns IP-adress).

## âš™ï¸ Tekniska Detaljer

*   **Frontend:** Streamlit (Python)
*   **Backend:** Microsoft SQL Server (Docker)
*   **AI:** Ollama (lokal LLM)
*   **Datagenerering:** Python-skript som simulerar data fÃ¶r larm, produktion och OEE.
*   **Visualisering:** Plotly (Python)
*   **NÃ¤tverk:** Appen anvÃ¤nder Host Mode fÃ¶r bÃ¤sta nÃ¤tverksprestanda (men se till att din brandvÃ¤gg tillÃ¥ter trafik).

## ğŸ“œ Licens
Detta projekt Ã¤r licensierat under [MIT-licensen](https://opensource.org/licenses/MIT) - se `LICENSE`-filen fÃ¶r mer detaljer.

## ğŸ“¸ SkÃ¤rmdumpar

![HuvudskÃ¤rmen pÃ¥ Godisfabrik 4.0](screenshots/sida1.png)
![OEE-dashboard](screenshots/oee.png)
![Larm rapport](screenshots/larm.png)
![Chat](screenshots/chat.png)
![Maskin 1 Ã¶versikt](screenshots/maskin_1_1_Ã¶versikt.png)
![Maskin chat](screenshots/maskin_1_1_chat.png)

---
